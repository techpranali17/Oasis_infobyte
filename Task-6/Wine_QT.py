# -*- coding: utf-8 -*-
"""Copy of Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1esPP_MFZ5BBA1F0B2Xw4wEMJ0Q9YXn1D
"""

import numpy as np
import pandas as pd

dataset_url="https://drive.google.com/file/d/19lw2ZiOM_DJ9-WD5Vt9OFUrqQBFobrzH/view?usp=drive_link"

df = pd.read_csv(dataset_url)

import pandas as pd

# Load the dataset
df = pd.read_csv('WineQT.csv')

# Handle missing values
df = df.dropna()  # or df.fillna(df.mean(), inplace=True)

# Remove duplicates
df = df.drop_duplicates()

# Correct data types
df['quality'] = df['quality'].astype(int)  # Example: converting 'quality' to integer

# Display the cleaned dataset
print(df.info())
print(df.head())

import seaborn as sns
import matplotlib.pyplot as plt

print(df.head())

print(df.dtypes)

print(df.describe())

df.hist(bins=20, figsize=(20, 15))
plt.show()

for column in df.select_dtypes(include=[np.number]).columns:
    sns.boxplot(x=df[column])
    plt.title(f'Box plot of {column}')
    plt.show()

from sklearn.preprocessing import StandardScaler

features = df.drop('quality', axis=1)
target = df['quality']

scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)

scaled_df = pd.DataFrame(scaled_features, columns=features.columns)
scaled_df['quality'] = target.values

print(scaled_df.head())

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

X = scaled_df.drop('quality', axis=1)
y = scaled_df['quality']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='weighted')
    recall = recall_score(y_test, y_pred, average='weighted')
    f1 = f1_score(y_test, y_pred, average='weighted')
    return accuracy, precision, recall, f1

rf_model = RandomForestClassifier(random_state=42)
rf_accuracy, rf_precision, rf_recall, rf_f1 = evaluate_model(rf_model, X_train, X_test, y_train, y_test)

sgd_model = SGDClassifier(random_state=42)
sgd_accuracy, sgd_precision, sgd_recall, sgd_f1 = evaluate_model(sgd_model, X_train, X_test, y_train, y_test)

svc_model = SVC(random_state=42)
svc_accuracy, svc_precision, svc_recall, svc_f1 = evaluate_model(svc_model, X_train, X_test, y_train, y_test)

print("Random Forest - Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}".format(rf_accuracy, rf_precision, rf_recall, rf_f1))
print("SGD - Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}".format(sgd_accuracy, sgd_precision, sgd_recall, sgd_f1))
print("SVC - Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}".format(svc_accuracy, svc_precision, svc_recall, svc_f1))

from sklearn.model_selection import cross_val_score

def cross_validate_model(model, X, y, cv=5):
    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy')
    precision = cross_val_score(model, X, y, cv=cv, scoring='precision_weighted')
    recall = cross_val_score(model, X, y, cv=cv, scoring='recall_weighted')
    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1_weighted')
    return accuracy.mean(), precision.mean(), recall.mean(), f1.mean()

rf_cv_accuracy, rf_cv_precision, rf_cv_recall, rf_cv_f1 = cross_validate_model(rf_model, X, y)

sgd_cv_accuracy, sgd_cv_precision, sgd_cv_recall, sgd_cv_f1 = cross_validate_model(sgd_model, X, y)

svc_cv_accuracy,svc_cv_precision,svc_cv_recall,svc_cv_f1 = cross_validate_model(svc_model, X, y)

print("SVC (CV) - Accuracy: {:.2f}, Precision: {:.2f}, Recall: {:.2f}, F1 Score: {:.2f}".format(svc_cv_accuracy, svc_cv_precision, svc_cv_recall, svc_cv_f1))

df.hist(bins=20, figsize=(20,15))
plt.show()

df.hist(bins=20, figsize=(20, 15))
plt.show()

plt.figure(figsize=(12, 8))
sns.boxplot(data=df)
plt.xticks(rotation=90)
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Assuming you have a trained RandomForest model
X = df.drop('quality', axis=1)
y = df['quality']

# Train the model (example)
model = RandomForestClassifier()
model.fit(X, y)

# Get feature importances
importances = model.feature_importances_
indices = np.argsort(importances)[::-1]

# Plot feature importances
plt.figure(figsize=(12, 8))
plt.title("Feature Importances")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Plot predictions vs actual values
plt.figure(figsize=(12, 8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted Wine Quality')
plt.show()

# Calculate and print the mean squared error
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')